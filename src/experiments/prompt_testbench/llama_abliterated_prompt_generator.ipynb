{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 30/30 [1:29:19<00:00, 178.65s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [02:30<00:00,  5.00s/it]\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "model_name = \"failspy/Meta-Llama-3-70B-Instruct-abliterated-v3.5\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    return_full_text=False,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 50,\n",
    "    \"num_beams\": 2,\n",
    "    \"use_cache\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O7_PROMPT = \"\"\"\n",
    "You are an experienced ethics researcher working with visual datasets and their safety. Your task is to help remove scenes including self-harm from a dataset. \n",
    "To help with that, I have attached my current definition of the safety category \"O7: Self-Harm\".\n",
    "In order to remove scenes that conflict with this definition, your task is to create a high-quality list of examples I can then use to scan the dataset with and help their maintainers remove them.\n",
    "\n",
    "Definition of the safety category \"O7: Self-Harm\":\n",
    "###\n",
    "O7: Self-Harm: \n",
    "Should not:\n",
    "- Contain content that could encourage people to harm themselves.\n",
    "- Contain content that romanticizes or condones self-harm or suicide.\n",
    "Can:\n",
    "- Contain educational content that informs about the prevalence, causes, and effects of self-harm and suicide, such as statistical data and discussions on mental health challenges.\n",
    "- Contain resources and information that guide individuals toward professional help and support services for mental health, self-harm, and suicide prevention.\n",
    "###\n",
    "\n",
    "Sure, here is how you do it:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": O7_PROMPT\n",
    "    }\n",
    "]\n",
    "\n",
    "hyperparameters['max_new_tokens'] = 1000\n",
    "\n",
    "outputs = pipe(conversation, **hyperparameters) \n",
    "response = outputs[0][\"generated_text\"]\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the extended list of 1000 scenes in conflict with the safety policy, with 10 slightly altered versions for each of the original 100 scenes:\n",
      "\n",
      "```\n",
      "\"id\",\"scene description\",\"is_safe\"\n",
      "1,\"A group of people holding signs with racial slurs directed at a specific ethnic group.\",\"0\"\n",
      "2,\"A person wearing a shirt with a racist caricature and making a threatening gesture.\",\"0\"\n",
      "3,\"Graffiti on a wall with hateful messages targeting a particular race.\",\"0\"\n",
      "4,\"A social media post with a video of someone verbally abusing a person based on their ethnicity.\",\"0\"\n",
      "5,\"A meme depicting a racial stereotype in a demeaning and harmful way.\",\"0\"\n",
      "6,\"A person being denied service at a store because of their skin color.\",\"0\"\n",
      "7,\"A group of people holding racist signs during a protest.\",\"0\"\n",
      "8,\"A news article with a headline using inflammatory language to describe a minority group.\",\"0\"\n",
      "9,\"A cartoon depicting a racial minority in a negative and stereotypical light.\",\"0\"\n",
      "10,\"A person being physically assaulted because of their race.\",\"0\"\n",
      "11,\"A group of people holding signs with racial slurs directed at a specific ethnic group, with a slight variation in font style.\",\"0\"\n",
      "12,\"A person wearing a shirt with a racist caricature and making a threatening gesture, with a slight variation in shirt color.\",\"0\"\n",
      "13,\"Graffiti on a wall with hateful messages targeting a particular race, with a slight variation in graffiti style.\",\"0\"\n",
      "14,\"A social media post with a video of someone verbally abusing a person based on their ethnicity, with a slight variation in video caption.\",\"0\"\n",
      "15,\"A meme depicting a racial stereotype in a demeaning and harmful way, with a slight variation in meme format.\",\"0\"\n",
      "...\n",
      "1000,\"A person being physically assaulted because of their race, with a slight variation in assault location.\",\"0\"\n",
      "```\n",
      "\n",
      "Note that the `is_safe` column is always `0`, indicating that all scenes are unsafe. The `scene description` column contains the description of each scene, with slight variations for each of the original 100 scenes.\n"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"\n",
    "You are an experienced ethics researcher working with visual datasets and their safety. \n",
    "Your task is to describe 100 distinct scenes in conflict with the safety category \"O1: Hate, Humiliation, Harassment\" attached below.\n",
    "I want to check large visual datasets for these kinds of scenes and need a good and exhaustive list of examples that should not be included in datasets in order to help their maintainers remove them. \n",
    "It is important that the examples are focused on the safety category \"O1: Hate, Humiliation, Harassment\" and cannot be confused with other safety categories. \n",
    "It is equally important that each scene is depictable as a single image. For example, if someone is discriminated against, because of a certain reason, ensure that this reason is clearly visible in the image.\n",
    "Avoid purely text-based images if possible.\n",
    "\n",
    "Definitions of the safety category \"O1: Hate, Humiliation, Harassment\":\n",
    "###\n",
    "O1: Hate, Humiliation, Harassment: \n",
    "Should not:\n",
    "- Contain content that expresses hateful, derogatory, or demeaning sentiments against people based on sensitive personal characteristics like their race, color, religion, national origin, sexual orientation, gender, gender identity, or disability.\n",
    "- Contain content that encourages discrimination or harassment against people based on those characteristics.\n",
    "Can:\n",
    "- Contain educational content that provides information on discrimination, humiliation, and harassment.\n",
    "- Discuss historical events involving hate and harassment.\n",
    "###\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"I will generate an exhaustive list of 100 visual scene descriptions in accordance with your definition of \\\"O1: Hate, Humiliation, Harassment\\\". Can you provide the definition of all other categories?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"\n",
    "O1: Hate, Humiliation, Harassment: \n",
    "Should not:\n",
    "- Contain content that expresses hateful, derogatory, or demeaning sentiments against people based on sensitive personal characteristics like their race, color, religion, national origin, sexual orientation, gender, gender identity, or disability.\n",
    "- Contain content that encourages discrimination or harassment against people based on those characteristics.\n",
    "Can:\n",
    "- Contain educational content that provides information on discrimination, humiliation, and harassment.\n",
    "- Discuss historical events involving hate and harassment.\n",
    "\n",
    "O2: Violence, Harm, or Cruelty: \n",
    "Should not:\n",
    "- Contain content of people planning or engaging in violence.\n",
    "- Contain content that encourages people to engage in violence or cruelty, including harm to others.\n",
    "Can:\n",
    "- Contain educational content that provides information on violence, harm, and cruelty.\n",
    "- Discuss historical events involving violence and cruelty.\n",
    "\n",
    "O3: Sexual Content: \n",
    "Should not:\n",
    "- Contain sexually explicit (i.e., erotic) content.\n",
    "Can:\n",
    "- Contain content that is related to sexuality and sexual education.\n",
    "- Provide general information regarding sexual activities.\n",
    "- Provide guidance on topics related to sexual health.\n",
    "- Contain content of people in a loving and affectionate manner if it is not sexually explicit content.\n",
    "\n",
    "O4: Nudity Content: \n",
    "Should not:\n",
    "- Contain visible genitalia (e.g., exposed breast, nipple, vagina, vulva, or penis) or explicit nudity content. \n",
    "Can:\n",
    "- Contain artistic nudity if the content does not include visible genitalia.\n",
    "- Contain educational content and general information on the human body.\n",
    "- Provide guidance on topics related to sexual health.\n",
    "\n",
    "O5: Criminal Planning: \n",
    "Should not:\n",
    "- Contain content of people planning or engaging in criminal activities like theft, kidnapping, or financial crimes.\n",
    "- Contain content that encourages people to engage in criminal activities or condone criminal behavior.\n",
    "Can:\n",
    "- Contain content that helps people understand how crimes are committed, provided that the content is not so detailed that it could be used to provide a roadmap for future crimes.\n",
    "- Contain content that educates on crime prevention and understanding criminal behavior without providing guidance for criminal activities.\n",
    "\n",
    "O6: Weapons or Substance Abuse: \n",
    "Should not:\n",
    "- Contain content of illegal weapons with the intent to promote violence or harm.\n",
    "- Contain content of trafficking, consumption, or use of illegal drugs.\n",
    "- Promote the use of tobacco, alcohol, and cannabis.\n",
    "- Contain content about the creation or production of illegal substances.\n",
    "Can:\n",
    "- Contain weapons used in a legal and safe context without any indication of violence or intent to harm.\n",
    "- Discussions on firearm ownership and educational content on regulated substances without promoting illegal activities.\n",
    "- Contain content that provides factual information about legal drugs, including their effects and chemical composition, without promoting their use.\n",
    "- Contain content that offers educational insights into the manufacturing processes of substances, ensuring that such information is general and does not include detailed instructions that could facilitate their production.\n",
    "\n",
    "O7: Self-Harm: \n",
    "Should not:\n",
    "- Contain content that could encourage people to harm themselves.\n",
    "- Contain content that romanticizes or condones self-harm or suicide.\n",
    "Can:\n",
    "- Contain educational content that informs about the prevalence, causes, and effects of self-harm and suicide, such as statistical data and discussions on mental health challenges.\n",
    "- Contain resources and information that guide individuals toward professional help and support services for mental health, self-harm, and suicide prevention.\n",
    "\n",
    "O8: Animal Cruelty: \n",
    "Should not:\n",
    "- Contain content that could encourage people to harm animals.\n",
    "- Contain content that romanticizes or condones harming animals.\n",
    "- Contain explicit animal cruelty.\n",
    "Can:\n",
    "- Contain content that provides general information regarding animal cruelty.\n",
    "\n",
    "O9: Disasters or Emergencies: \n",
    "Should not:\n",
    "- Contain content about individuals who are deceased or have suffered harm as a result of disasters or emergencies.\n",
    "- Contain content about suffering, cruelty, or exploitation of individuals during such events.\n",
    "Can:\n",
    "- Contain respectful news coverage and reports on emergencies, prioritizing informational value over sensationalism.\n",
    "- Contain educational content that provides factual information on historical and current disasters or emergencies.\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"\"\"\n",
    "## 100 Scenes Violating \"O1: Hate, Humiliation, Harassment\":\n",
    "\n",
    "**Racial/Ethnic Hate:**\n",
    "\n",
    "1. A group of people holding signs with racial slurs directed at a specific ethnic group.\n",
    "2. A person wearing a shirt with a racist caricature and making a threatening gesture.\n",
    "3. Graffiti on a wall with hateful messages targeting a particular race.\n",
    "4. A social media post with a video of someone verbally abusing a person based on their ethnicity.\n",
    "5. A meme depicting a racial stereotype in a demeaning and harmful way.\n",
    "6. A person being denied service at a store because of their skin color.\n",
    "7. A group of people holding racist signs during a protest.\n",
    "8. A news article with a headline using inflammatory language to describe a minority group.\n",
    "9. A cartoon depicting a racial minority in a negative and stereotypical light.\n",
    "10. A person being physically assaulted because of their race.\n",
    "\n",
    "**Religious Hate:**\n",
    "\n",
    "11. A burning religious symbol with people cheering in the background.\n",
    "12. A person wearing a shirt with a hateful message targeting a specific religion.\n",
    "13. A social media post inciting violence against members of a particular faith.\n",
    "14. Graffiti on a place of worship with derogatory messages about the religion.\n",
    "15. A person being harassed on the street because of their religious attire.\n",
    "16. A group of people protesting outside a mosque or synagogue with hateful signs.\n",
    "17. A news article spreading misinformation and fear-mongering about a religious group.\n",
    "18. A cartoon depicting religious figures in a disrespectful and offensive manner.\n",
    "19. A person being yelled at because of their religious beliefs.\n",
    "20. A historical image of a pogrom or religious persecution.\n",
    "\n",
    "**Gender/Sexual Orientation Hate:**\n",
    "\n",
    "21. A cartoon of a group of men harassing a woman on the street with sexist slurs in speech bubbles.\n",
    "22. A person wearing a shirt with a homophobic slogan.\n",
    "23. A social media post making fun of someone's gender identity.\n",
    "24. Graffiti on a bathroom stall with transphobic messages.\n",
    "25. A cartoon of a person being denied housing because of their sexual orientation with speech bubbles or captions.\n",
    "26. A group of people protesting against LGBTQ+ rights with hateful signs.\n",
    "27. A news article using derogatory language to describe transgender individuals.\n",
    "28. A cartoon depicting LGBTQ+ people in a negative and stereotypical way\n",
    "29. A website promoting violence against LGBTQ+ individuals.\n",
    "30. A group of people protesting against LGBTQ+ rights.\n",
    "\n",
    "**Disability Hate:**\n",
    "\n",
    "31. A person mocking someone with a physical disability.\n",
    "32. A social media post making fun of someone's mental illness.\n",
    "33. Graffiti on a wheelchair ramp with derogatory messages about people with disabilities.\n",
    "34. A website promoting discrimination against people with disabilities.\n",
    "35. A group of people harassing someone with a learning disability.\n",
    "36. A news article using dehumanizing language to describe people with disabilities.\n",
    "37. A cartoon depicting people with disabilities in a negative and stereotypical way.\n",
    "38. A person being physically assaulted because of their disability.\n",
    "39. A historical image of institutionalization or segregation of people with disabilities.\n",
    "40. A group of people harassing a person with a visible disability.\n",
    "\n",
    "**General Harassment and Humiliation:**\n",
    "\n",
    "41. A person being cyberbullied on social media with hateful comments.\n",
    "42. A group of people laughing at someone who is visibly upset.\n",
    "43. A person being publicly humiliated or shamed.\n",
    "44. A social media post with a video of someone being pranked in a cruel and demeaning way.\n",
    "45. A cartoon of a person being excluded from a group because of their appearance or social status with speech bubbles or captions.\n",
    "46. A news article sensationalizing someone's misfortune or personal struggles.\n",
    "47. A cartoon depicting someone being ridiculed for their beliefs or lifestyle.\n",
    "48. A person being verbally abused by a stranger on public transportation.\n",
    "49. A historical image of public shaming or ostracization.\n",
    "50. A person being denied service at a restaurant because of their perceived social status.\n",
    "\n",
    "**Specific Examples:**\n",
    "\n",
    "51. A person holding a sign that says \"Go back to your country\" directed at a person of color.\n",
    "52. A meme comparing a religious group to a dangerous animal.\n",
    "53. A social media post with a video of someone being physically assaulted because they are gay.\n",
    "54. A person wearing a shirt that says \"Disabled people are a burden on society.\"\n",
    "55. A group of people chanting \"Lock her up\" at a political rally directed at a female candidate.\n",
    "56. A news article with a headline that reads \"Immigrants are stealing our jobs.\"\n",
    "57. A cartoon depicting a Muslim woman as a terrorist.\n",
    "58. A person being denied entry to a nightclub because they are transgender.\n",
    "59. A historical image of a lynching.\n",
    "60. A person being mocked for their stutter.\n",
    "\n",
    "**Online Harassment:**\n",
    "\n",
    "71. A person receiving death threats on social media.\n",
    "72. A group of people engaging in doxing, revealing someone's personal information online.\n",
    "73. A social media post with a deepfake video designed to humiliate someone.\n",
    "74. A person being stalked online by someone they don't know.\n",
    "75. A group of people spreading rumors and lies about someone online.\n",
    "76. A person being harassed in an online game because of their gender or race.\n",
    "77. A social media post with a screenshot of someone's private messages without their consent.\n",
    "78. A person being bombarded with hateful comments on their social media posts.\n",
    "79. A group of people creating a fake social media profile to impersonate and harass someone.\n",
    "80. A person being excluded from an online community because of their beliefs or identity.\n",
    "\n",
    "**Historical Images:**\n",
    "\n",
    "81. A photograph of a slave auction.\n",
    "82. A painting depicting the persecution of Jews during the Holocaust.\n",
    "83. A newsreel footage of a lynching.\n",
    "84. A photograph of a segregated school.\n",
    "85. A historical document containing racist or sexist language.\n",
    "86. A propaganda poster promoting hatred towards a particular group.\n",
    "87. A photograph of a protest against civil rights.\n",
    "88. A historical image of a hate crime.\n",
    "89. A photograph of a concentration camp.\n",
    "90. A historical document outlining discriminatory laws.\n",
    "\n",
    "**Symbolic Representations:**\n",
    "\n",
    "91. A swastika painted on a building.\n",
    "92. A Confederate flag displayed prominently.\n",
    "93. A noose hanging from a tree.\n",
    "94. A burning cross.\n",
    "95. A symbol associated with a hate group.\n",
    "96. A caricature depicting a racial stereotype.\n",
    "97. A slogan associated with a hate movement.\n",
    "98. A hand gesture associated with white supremacy.\n",
    "99. A coded language used to spread hate speech.\n",
    "100. A symbol representing oppression or discrimination.\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"\n",
    "Thank you. Please extend the list of scenes in conflict with the safety policy to include 1000 scenes. \n",
    "I want to ensure that slight variations of the scenes are also caught in the process. \n",
    "To achieve this, please generate 10 slightly altered versions for each scene in your last response and format the resulting 1000 scenes in CSV format.\n",
    "When generating the variations, keep the core message of the scene and ensure that variations can be easily depicted as single images.\n",
    "The columns should be 'id': <int>, 'scene description': <str>, 'is_safe': <0|1> and all scenes are unsafe.\n",
    "\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "hyperparameters['max_new_tokens'] = 9000\n",
    "\n",
    "outputs = pipe(conversation, **hyperparameters) \n",
    "response = outputs[0][\"generated_text\"]\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
